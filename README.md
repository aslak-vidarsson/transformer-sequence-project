# Project: Transformer Model to Predict Sequences

The results can be found in **FINAL_DRAFT_PROJECT2.ipynb**. This was a two-week project in the course "Introduction to numerical calculations" at NTNU by Aslak Vidarsson Homme and Hallstein Olesønn Hagaseth, both students at NTNU.

## Project Highlights

- Implemented a **transformer model from scratch in Python**, using only **NumPy** and **Matplotlib**—no AI libraries.
- Tested the model on the following tasks: sorting, addition and generate Shakespeare-like text!

## Key Results

### Sequence Sorting
- **Adam optimizer:** 99.84% of sequences correctly sorted.  
- **Steepest Descent:** 1.64% of sequences correctly sorted.

### Addition of Two Two-Digit Numbers
- Adam converged quickly below tolerance and achieved **99.49% accuracy**.  
- Steepest Descent converged slowly, with **0.99% accuracy**.

These results highlight the efficiency of the Adam optimizer and the effectiveness of a from-scratch transformer implementation.

## Additional Experiment: Shakespeare Text Generation

We also experimented with generating text in the style of Shakespeare.  
Example output:
Thou shall nothitite Madyed him; and fack of mece off of his peasing not ficers.

CORIOLANUS:
Nowes day, let the geas!

LARTIUS:
Your none his Nay.

When the noner the say the to stendstall,
They!
They thou war
Mostiong the say none should men: May like set his glet your burave belas!
That ast, shere should me my foulamm the what drequechare itild afeplers,
That think mostichen grebshound him: Tis gindeed beas the seas!
Toold that the rep in say!
Aufid bleast lep diet her thould cetscest: ay, a 


